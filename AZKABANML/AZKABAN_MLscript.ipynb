{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f40377a2",
   "metadata": {},
   "source": [
    "\n",
    "Originally created by Chelsey McGowan-Yallop, SAMS-UHI (sa06cm@sams.ac.uk)\n",
    "\n",
    "Modified by Muriel Dunn for fish mix analysis\n",
    "\n",
    "This script uses model-predicted TS(f) spectra to train a machine learning\n",
    "classifier, performs nested cross-validation, applies the classifier to\n",
    "measured TS(f) spectra and outputs results files.\n",
    "\n",
    "To use a different classifier, see the list of supported classifiers at:\n",
    "https://github.com/hyperopt/hyperopt-sklearn and set as clf.\n",
    "\n",
    "Sometimes the initial hyperparameter configuration selected by HyperOpt in each\n",
    "split in the outer loop will be unsuccessful and all trials will fail. The\n",
    "retry decorator forces it to try again until retry_limit is reached.\n",
    "\n",
    "OUTPUT FILES:\n",
    "    _NestedCV.pkl contains results of nested cross-validation procedure\n",
    "    _Predictions.pkl contains measured TS(f) spectra with predicted labels\n",
    "    _BestParams.pkl contains the optimal hyperparameters for the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab9c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import hyperopt\n",
    "from hyperopt import tpe\n",
    "\n",
    "from hpsklearn import HyperoptEstimator, k_neighbors_classifier, svc, lightgbm_classification, gaussian_nb, any_preprocessing,min_max_scaler, normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from datetime import timedelta\n",
    "from tenacity import retry, stop_after_attempt\n",
    "\n",
    "import sys, errno  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2319ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- USER-DETERMINED PARAMETERS -----------------------------------------------\n",
    "path = 'C:/Users/mbd/OneDrive - Akvaplan-niva AS/PhD-APN/ChaptersandExperiments/AZKABAN-light/ZoopMix_paper/'\n",
    "\n",
    "# pickled FILES FROM ECHOVIEW\n",
    "df_SED = pd.read_pickle(path+'df_SED.pkl')\n",
    "df_track = pd.read_pickle(path+'df_track.pkl')\n",
    "df_trackavg = pd.read_pickle(path+'df_trackavg.pkl')\n",
    "\n",
    "\n",
    "# SCATTERING MODEL\n",
    "model_path = path+\"AZKABAN_ZoopMix_data_newcopepod.feather\" # Path to model outputs\n",
    "n_models_per_species = 1000 # No. of models per species\n",
    "\n",
    "# CLASSIFIER\n",
    "unique_id = '12-06-2023_kNN_AZKABAN' # Unique ID for output file paths\n",
    "clf = k_neighbors_classifier(unique_id)  # Classifier\n",
    "\n",
    "# NESTED CROSS-VALIDATION\n",
    "preprocessing = [] # List of sklearn pre-processing modules\n",
    "ex_preprocessing = [] # As above, see help(HyperoptEstimator) for info\n",
    "n_splits = 10 # Value of k for k-fold cross-validation in outer loop\n",
    "n_folds = 10 # Value of k for k-fold cross-validation in inner loop\n",
    "max_evals = 50 # No. of HyperOpt trials\n",
    "timeout = 300 # HyperOpt trial timeout (seconds)\n",
    "n_jobs = -1 # No. of jobs to run in parallel; -1 uses all processors\n",
    "retry_limit = 10 # No. of times to retry before failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e54ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(retry_limit))\n",
    "def nested_cv(X, y, model, n_splits, n_folds, unique_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function performs nested cross-validation with Bayesian hyperparameter\n",
    "    optimisation. It uses stratified k-fold cross-validation in both the inner\n",
    "    and outer loops. After each outer loop, it outputs the results to a .pkl\n",
    "    file. As there is an element of randomness to the optimisation procedure,\n",
    "    sometimes all trials will fail. If you re-run the script, it will import\n",
    "    the incomplete .pkl file and try again.\n",
    "    \n",
    "    Note that this is a modified version that uses F1 score as the evaluation\n",
    "    metric. It also calculates class-specific F1 scores and confusion matrices,\n",
    "    which are added to the output dataframe.\n",
    "    \n",
    "    PARAMETERS:\n",
    "        X: data minus labels\n",
    "        y: labels\n",
    "        model: HyperoptEstimator object\n",
    "        n_splits: # of splits to use in outer K-fold cross-validation\n",
    "        n_folds: # of folds to use in inner K-fold cross-validation\n",
    "        unique_id: Unique name string for file output path\n",
    "    \"\"\"\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=n_splits,\n",
    "                         shuffle=True,\n",
    "                         random_state=42) # Outer CV\n",
    "    \n",
    "    i_start = 0\n",
    "    i_list = []\n",
    "    results_df = None\n",
    "    cv_path = unique_id + '_NestedCV.pkl'\n",
    "        \n",
    "    if os.path.isfile(cv_path) == True: # If CV is incomplete, resume\n",
    "        results_df = pd.read_pickle(cv_path)\n",
    "        i_start = results_df.Outer_fold.max() + 1\n",
    "        print('Resuming cross-validation from fold ' + str(i_start + 1))\n",
    "        \n",
    "    # Generate indices to split data by StratifiedKFold\n",
    "    # Append indices for each fold to list    \n",
    "    for tr_i, te_i in cv.split(X,y):\n",
    "        i_list.append([tr_i, te_i])\n",
    "    \n",
    "    # For each fold...\n",
    "    for i in range(i_start, len(i_list)):\n",
    "        results_list = []\n",
    "        print('Beginning fold ' + str(i+1) + ' of ' + str(len(i_list)))\n",
    "        \n",
    "        # Split data into training and test tests\n",
    "        X_train = X.loc[X.index.intersection(i_list[i][0])]\n",
    "        y_train = y.loc[y.index.intersection(i_list[i][0])]\n",
    "        X_test = X.loc[X.index.intersection(i_list[i][1])]\n",
    "        y_test = y.loc[y.index.intersection(i_list[i][1])]\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        # Fit the HyperoptEstimator to training data (optimise model)\n",
    "        model.fit(X_train,\n",
    "                  y_train,\n",
    "                  n_folds=n_folds, # Inner stratified k-fold CV\n",
    "                  cv_shuffle=True)\n",
    "        \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "\n",
    "        # Use optimised model to predict labels for test data\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = f1_score(y_test, y_pred, average='weighted') # Evaluate\n",
    "        \n",
    "        # Everything below: formats and/or calculates results for output file\n",
    "        sorted_labels = np.sort(y_test.unique())\n",
    "        unweighted_score = f1_score(y_test, y_pred,\n",
    "                                    average=None,\n",
    "                                    labels=sorted_labels)\n",
    "        c_matrix = confusion_matrix(y_test, y_pred,\n",
    "                                    labels=sorted_labels)\n",
    "\n",
    "        for trial in range(len(model.trials.trials)):\n",
    "                if model.trials.trials[trial].get('result').get('status') == 'ok':\n",
    "                    trial_loss = model.trials.trials[trial].get('result').get('loss')\n",
    "                    trial_duration = model.trials.trials[trial].get('result').get('duration')\n",
    "                else:\n",
    "                    trial_loss = np.nan\n",
    "                    trial_duration = np.nan\n",
    "            \n",
    "                results_list.append([i,\n",
    "                                     score,\n",
    "                                     unweighted_score,\n",
    "                                     le.inverse_transform(sorted_labels),\n",
    "                                     c_matrix,\n",
    "                                     duration,\n",
    "                                     trial,\n",
    "                                     trial_loss,\n",
    "                                     trial_duration])\n",
    "        \n",
    "        append_df = pd.DataFrame(results_list,\n",
    "                                 columns=['Outer_fold',\n",
    "                                          'Outer_score',\n",
    "                                          'Outer_unweighted_scores',\n",
    "                                          'Outer_unweighted_score_labels',\n",
    "                                          'Outer_confusion_matrix',\n",
    "                                          'Outer_training_duration',\n",
    "                                          'Trial',\n",
    "                                          'Trial_loss',\n",
    "                                          'Trial_duration'])\n",
    "        if i == i_start:\n",
    "            if results_df is not None:\n",
    "                final_df = pd.concat([results_df,\n",
    "                                      append_df],\n",
    "                                      ignore_index=True)\n",
    "            else:\n",
    "                final_df = append_df\n",
    "            final_df.to_pickle(cv_path)\n",
    "        \n",
    "        else:\n",
    "            results_df = pd.read_pickle(cv_path)\n",
    "            final_df = pd.concat([results_df,\n",
    "                                  append_df],\n",
    "                                  ignore_index=True)\n",
    "            final_df.to_pickle(cv_path)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0cfb65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function for HyperOpt.\n",
    "    Uses F1 score instead of accuracy score, as the latter is inappropriate\n",
    "    for multi-class classification.\n",
    "    \"\"\"\n",
    "    return 1.0 - f1_score(y_true, y_pred, average='weighted')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b050fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():    \n",
    "    # -- IMPORT FILES ---------------------------------------------------------\n",
    "     \n",
    "    measured_df = tsf_import(tsf_path) # Measured TS(f)\n",
    "    model_df = pd.read_feather(model_path) # Modelled TS(f)\n",
    "\n",
    "    # -- RESTRUCTURE MODEL DATA -----------------------------------------------\n",
    "\n",
    "    measured_frequency = [float(i) for i in measured_df.columns.values[3:-1]]\n",
    "    n_model_f_bins = len(model_df.freq.unique()) # No. freq bins in model data\n",
    "    n_species = len(model_df.spec.unique()) # No. species in model data\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    start_lim = 0\n",
    "    stop_lim = n_model_f_bins\n",
    "\n",
    "    for i in range(n_species * n_models_per_species):\n",
    "        TS_array = model_df.TS[start_lim:stop_lim].values\n",
    "        species_label = model_df.spec[start_lim:stop_lim].values[0]\n",
    "\n",
    "        X_list.append(TS_array)\n",
    "        y_list.append(species_label)\n",
    "\n",
    "        start_lim += n_model_f_bins\n",
    "        stop_lim += n_model_f_bins\n",
    "\n",
    "    model_df = pd.DataFrame(X_list, columns=measured_frequency)\n",
    "    model_df['Species'] = y_list\n",
    "\n",
    "    # -- WRANGLE DATA ---------------------------------------------------------\n",
    "\n",
    "    le = LabelEncoder() # Maps labels -> int (e.g. Copepods -> 0, Krill -> 1)\n",
    "    model_df['Species'] = le.fit_transform(model_df.Species)\n",
    "    X = model_df.iloc[:,:-1] # Features, TS(f) only\n",
    "    y = model_df.Species # Labels\n",
    "\n",
    "    if min_range != None:\n",
    "        measured_df = measured_df[measured_df.Range > min_range]\n",
    "\n",
    "    if max_range != None:\n",
    "        measured_df = measured_df[measured_df.Range < max_range]\n",
    "\n",
    "    measured_X = measured_df.iloc[:, 3:-1] # Features, TS(f) only\n",
    "\n",
    "    # -- NESTED CROSS-VALIDATION ----------------------------------------------\n",
    "\n",
    "    model = HyperoptEstimator(classifier = clf,\n",
    "                              preprocessing = preprocessing,\n",
    "                              ex_preprocs = ex_preprocessing,\n",
    "                              algo = tpe.suggest,\n",
    "                              trial_timeout = timeout,\n",
    "                              loss = f1_loss,\n",
    "                              max_evals = max_evals,\n",
    "                              n_jobs = n_jobs)\n",
    "\n",
    "    nested_cv(X, y, model, n_splits, n_folds, unique_id)\n",
    "\n",
    "    # -- RETRAIN MODEL --------------------------------------------------------\n",
    "\n",
    "    print('Retraining model on full dataset')\n",
    "\n",
    "    model = HyperoptEstimator(classifier = clf,\n",
    "                              preprocessing = preprocessing,\n",
    "                              ex_preprocs = ex_preprocessing,\n",
    "                              algo = tpe.suggest,\n",
    "                              trial_timeout = timeout,\n",
    "                              loss = f1_loss,\n",
    "                              max_evals = max_evals,\n",
    "                              n_jobs = n_jobs)\n",
    "\n",
    "    model.fit(X, y, n_folds=n_folds, cv_shuffle=True)\n",
    "\n",
    "    # -- PREDICT CLASSES FOR NEW DATA -----------------------------------------\n",
    "\n",
    "    print('Classifying new data')\n",
    "\n",
    "    y_pred = model.predict(measured_X) # Predict classes for measured TS(f)\n",
    "    y_pred = le.inverse_transform(y_pred) # Transform labels back to species\n",
    "\n",
    "    # -- OUTPUT RESULTS -------------------------------------------------------\n",
    "\n",
    "    measured_df['Prediction'] = y_pred\n",
    "    measured_df.to_pickle(unique_id + '_Predictions.pkl')\n",
    "\n",
    "    with open(unique_id + '_BestParams.pkl', 'wb') as handle:\n",
    "        pickle.dump(model.best_model(), handle)\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a649e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
