{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f40377a2",
   "metadata": {},
   "source": [
    "\n",
    "Originally created by Chelsey McGowan-Yallop, SAMS-UHI (sa06cm@sams.ac.uk)\n",
    "\n",
    "Modified by Muriel Dunn for fish mix analysis\n",
    "\n",
    "This script uses model-predicted TS(f) spectra to train a machine learning\n",
    "classifier, performs nested cross-validation, applies the classifier to\n",
    "measured TS(f) spectra and outputs results files.\n",
    "\n",
    "To use a different classifier, see the list of supported classifiers at:\n",
    "https://github.com/hyperopt/hyperopt-sklearn and set as clf.\n",
    "\n",
    "Sometimes the initial hyperparameter configuration selected by HyperOpt in each\n",
    "split in the outer loop will be unsuccessful and all trials will fail. The\n",
    "retry decorator forces it to try again until retry_limit is reached.\n",
    "\n",
    "OUTPUT FILES:\n",
    "    _NestedCV.pkl contains results of nested cross-validation procedure\n",
    "    _Predictions.pkl contains measured TS(f) spectra with predicted labels\n",
    "    _BestParams.pkl contains the optimal hyperparameters for the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eab9c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "import hyperopt\n",
    "from hyperopt import tpe\n",
    "from hpsklearn import HyperoptEstimator, k_neighbors_classifier, svc, lightgbm_classification\n",
    "import lightgbm\n",
    "from datetime import timedelta\n",
    "from tenacity import retry, stop_after_attempt\n",
    "\n",
    "import sys, errno  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2319ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- USER-DETERMINED PARAMETERS -----------------------------------------------\n",
    "path = 'F:/Nya-AZKABAN/AZKABAN-fish2022/'\n",
    "\n",
    "# CLASSIFIER\n",
    "unique_id = '04-01-2023_kNN_AZKABAN' # Unique ID for output file paths\n",
    "clf = k_neighbors_classifier(unique_id)  # Classifier\n",
    "\n",
    "# NESTED CROSS-VALIDATION\n",
    "preprocessing = [] # List of sklearn pre-processing modules\n",
    "ex_preprocessing = [] # As above, see help(HyperoptEstimator) for info\n",
    "n_splits = 2 # Value of k for k-fold cross-validation in outer loop\n",
    "n_folds = 2 # Value of k for k-fold cross-validation in inner loop\n",
    "max_evals = 10 # No. of HyperOpt trials\n",
    "timeout = 600 # HyperOpt trial timeout (seconds)\n",
    "n_jobs = -1 # No. of jobs to run in parallel; -1 uses all processors\n",
    "retry_limit = 3 # No. of times to retry before failing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664acfb7",
   "metadata": {},
   "source": [
    "# Read the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "485f0cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pickle to open on Stokes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39c3cf",
   "metadata": {},
   "source": [
    "# Classification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16e54ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(retry_limit))\n",
    "def nested_cv(X, y, model, n_splits, n_folds, unique_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function performs nested cross-validation with Bayesian hyperparameter\n",
    "    optimisation. It uses stratified k-fold cross-validation in both the inner\n",
    "    and outer loops. After each outer loop, it outputs the results to a .pkl\n",
    "    file. As there is an element of randomness to the optimisation procedure,\n",
    "    sometimes all trials will fail. If you re-run the script, it will import\n",
    "    the incomplete .pkl file and try again.\n",
    "    \n",
    "    Note that this is a modified version that uses F1 score as the evaluation\n",
    "    metric. It also calculates class-specific F1 scores and confusion matrices,\n",
    "    which are added to the output dataframe.\n",
    "    \n",
    "    PARAMETERS:\n",
    "        X: data minus labels\n",
    "        y: labels\n",
    "        model: HyperoptEstimator object\n",
    "        n_splits: # of splits to use in outer K-fold cross-validation\n",
    "        n_folds: # of folds to use in inner K-fold cross-validation\n",
    "        unique_id: Unique name string for file output path\n",
    "    \"\"\"\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=n_splits,\n",
    "                         shuffle=True,\n",
    "                         random_state=42) # Outer CV\n",
    "    \n",
    "    i_start = 0\n",
    "    i_list = []\n",
    "    results_df = None\n",
    "    cv_path = unique_id + '_NestedCV.pkl'\n",
    "        \n",
    "    if os.path.isfile(cv_path) == True: # If CV is incomplete, resume\n",
    "        results_df = pd.read_pickle(cv_path)\n",
    "        i_start = results_df.Outer_fold.max() + 1\n",
    "        print('Resuming cross-validation from fold ' + str(i_start + 1))\n",
    "        \n",
    "    # Generate indices to split data by StratifiedKFold\n",
    "    # Append indices for each fold to list    \n",
    "    for tr_i, te_i in cv.split(X,y):\n",
    "        i_list.append([tr_i, te_i])\n",
    "    \n",
    "    # For each fold...\n",
    "    for i in range(i_start, len(i_list)):\n",
    "        results_list = []\n",
    "        print('Beginning fold ' + str(i+1) + ' of ' + str(len(i_list)))\n",
    "        \n",
    "        # Split data into training and test tests\n",
    "        X_train = X.loc[X.index.intersection(i_list[i][0])]\n",
    "        y_train = y.loc[y.index.intersection(i_list[i][0])]\n",
    "        X_test = X.loc[X.index.intersection(i_list[i][1])]\n",
    "        y_test = y.loc[y.index.intersection(i_list[i][1])]\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        # Fit the HyperoptEstimator to training data (optimise model)\n",
    "        model.fit(X_train,\n",
    "                  y_train,\n",
    "                  n_folds=n_folds, # Inner stratified k-fold CV\n",
    "                  cv_shuffle=True)\n",
    "        \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "\n",
    "        # Use optimised model to predict labels for test data\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = f1_score(y_test, y_pred, average='weighted') # Evaluate\n",
    "        \n",
    "        # Everything below: formats and/or calculates results for output file\n",
    "        sorted_labels = np.sort(y_test.unique())\n",
    "        unweighted_score = f1_score(y_test, y_pred,\n",
    "                                    average=None,\n",
    "                                    labels=sorted_labels)\n",
    "        c_matrix = confusion_matrix(y_test, y_pred,\n",
    "                                    labels=sorted_labels)\n",
    "\n",
    "        for trial in range(len(model.trials.trials)):\n",
    "                if model.trials.trials[trial].get('result').get('status') == 'ok':\n",
    "                    trial_loss = model.trials.trials[trial].get('result').get('loss')\n",
    "                    trial_duration = model.trials.trials[trial].get('result').get('duration')\n",
    "                else:\n",
    "                    trial_loss = np.nan\n",
    "                    trial_duration = np.nan\n",
    "            \n",
    "                results_list.append([i,\n",
    "                                     score,\n",
    "                                     unweighted_score,\n",
    "                                     le.inverse_transform(sorted_labels),\n",
    "                                     c_matrix,\n",
    "                                     duration,\n",
    "                                     trial,\n",
    "                                     trial_loss,\n",
    "                                     trial_duration])\n",
    "        \n",
    "        append_df = pd.DataFrame(results_list,\n",
    "                                 columns=['Outer_fold',\n",
    "                                          'Outer_score',\n",
    "                                          'Outer_unweighted_scores',\n",
    "                                          'Outer_unweighted_score_labels',\n",
    "                                          'Outer_confusion_matrix',\n",
    "                                          'Outer_training_duration',\n",
    "                                          'Trial',\n",
    "                                          'Trial_loss',\n",
    "                                          'Trial_duration'])\n",
    "        if i == i_start:\n",
    "            if results_df is not None:\n",
    "                final_df = pd.concat([results_df,\n",
    "                                      append_df],\n",
    "                                      ignore_index=True)\n",
    "            else:\n",
    "                final_df = append_df\n",
    "            final_df.to_pickle(cv_path)\n",
    "        \n",
    "        else:\n",
    "            results_df = pd.read_pickle(cv_path)\n",
    "            final_df = pd.concat([results_df,\n",
    "                                  append_df],\n",
    "                                  ignore_index=True)\n",
    "            final_df.to_pickle(cv_path)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0cfb65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function for HyperOpt.\n",
    "    Uses F1 score instead of accuracy score, as the latter is inappropriate\n",
    "    for multi-class classification.\n",
    "    \"\"\"\n",
    "    return 1.0 - f1_score(y_true, y_pred, average='weighted')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e84ae",
   "metadata": {},
   "source": [
    "# Script the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd8314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8da31061",
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_frequency = [float(i) for i in mix_df.columns.values]\n",
    "n_model_f_bins = len(measured_frequency) # No. freq bins in model data\n",
    "n_species = len(single_df.Species.unique()) # No. species in model data\n",
    "\n",
    "# -- WRANGLE DATA ---------------------------------------------------------\n",
    "\n",
    "le = LabelEncoder() # Maps labels -> int (e.g. Atlantic cod -> 0, Polar cod -> 1)\n",
    "single_df['Species_le'] = le.fit_transform(single_df.Species)\n",
    "X = single_df.iloc[:,:-2] # Features, TS(f) only\n",
    "y = single_df.Species_le # Labels\n",
    "\n",
    "model = HyperoptEstimator(classifier = clf,\n",
    "                          preprocessing = preprocessing,\n",
    "                          ex_preprocs = ex_preprocessing,\n",
    "                          algo = tpe.suggest,\n",
    "                          trial_timeout = timeout,\n",
    "                          loss_fn = f1_loss,\n",
    "                          max_evals = max_evals,\n",
    "                          n_jobs = n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f45af461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1/1 [10:03<00:00, 603.86s/trial, best loss=?]\n"
     ]
    },
    {
     "ename": "AllTrialsFailed",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAllTrialsFailed\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18284\\1298606489.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\hpsklearn\\estimator\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, EX_list, valid_size, n_folds, cv_shuffle, warm_start, random_state)\u001b[0m\n\u001b[0;32m    462\u001b[0m                 increment = min(self.fit_increment,\n\u001b[0;32m    463\u001b[0m                                 adjusted_max_evals - len(self.trials.trials))\n\u001b[1;32m--> 464\u001b[1;33m                 \u001b[0mfit_iter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincrement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_increment_dump_filename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\hpsklearn\\estimator\\estimator.py\u001b[0m in \u001b[0;36mfit_iter\u001b[1;34m(self, X, y, EX_list, valid_size, n_folds, cv_shuffle, warm_start, random_state)\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[1;31m# Workaround for rstate issue #35\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"rstate\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetfullargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                 hyperopt.fmin(_fn_with_timeout,\n\u001b[0m\u001b[0;32m    340\u001b[0m                               \u001b[0mspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                               \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mallow_trials_fmin\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fmin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m         return trials.fmin(\n\u001b[0m\u001b[0;32m    541\u001b[0m             \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfmin\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         return fmin(\n\u001b[0m\u001b[0;32m    672\u001b[0m             \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[1;31m# Only if there are some successful trail runs, return the best point in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;31m# the evaluation space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mspace_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36margmin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m         \u001b[0mbest_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"misc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vals\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;31m# unpack the one-element lists to values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mbest_trial\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    609\u001b[0m         ]\n\u001b[0;32m    610\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAllTrialsFailed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"result\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAllTrialsFailed\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b050fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_classify():    \n",
    "\n",
    "    \"\"\n",
    "    #path: srt. path to dataframes\n",
    "    #detections: str. Type of detections, SED, trackSED or trackavg.\n",
    "    \n",
    "    detections = 'SED'\n",
    "    path = './'\n",
    "\n",
    "    # -- IMPORT FILES ---------------------------------------------------------\n",
    "    if detections == 'SED':\n",
    "        labelled_df = pd.read_pickle(path+'single_SED_df.pkl')\n",
    "        mix_df = pd.read_pickle(path+'fm_SED_df.pkl')\n",
    "\n",
    "    elif detections == 'trackSED':\n",
    "        labelled_df = pd.read_pickle(path+'single_trackSED_df.pkl')\n",
    "        mix_df = pd.read_pickle(path+'fm_trackSED_df.pkl')\n",
    "\n",
    "    else:\n",
    "        labelled_df = pd.read_pickle(path+'single_trackavg_df.pkl')\n",
    "        mix_df = pd.read_pickle(path+'fm_trackavg_df.pkl')\n",
    "\n",
    "    # -- RESTRUCTURE MODEL DATA -----------------------------------------------\n",
    "\n",
    "    measured_frequency = [float(i) for i in mix_df.columns.values]\n",
    "    n_model_f_bins = len(measured_frequency) # No. freq bins in model data\n",
    "    n_species = len(labelled_df.Species.unique()) # No. species in model data\n",
    "\n",
    "    # -- WRANGLE DATA ---------------------------------------------------------\n",
    "\n",
    "    le = LabelEncoder() # Maps labels -> int (e.g. Copepods -> 0, Krill -> 1)\n",
    "    labelled_df['Species_le'] = le.fit_transform(labelled_df.Species)\n",
    "    X = labelled_df.iloc[:,14+1:-15-2] # Features, TS(f) only\n",
    "    y = labelled_df.Species_le # Labels\n",
    "\n",
    "\n",
    "    measured_X = mix_df # Features, TS(f) only\n",
    "\n",
    "    # -- NESTED CROSS-VALIDATION ----------------------------------------------\n",
    "\n",
    "    model = HyperoptEstimator(classifier = clf,\n",
    "                              preprocessing = preprocessing,\n",
    "                              ex_preprocs = ex_preprocessing,\n",
    "                              algo = tpe.suggest,\n",
    "                              trial_timeout = timeout,\n",
    "                              loss_fn = f1_loss,\n",
    "                              max_evals = max_evals,\n",
    "                              n_jobs = n_jobs)\n",
    "    model\n",
    "\n",
    "    nested_cv(X, y, model, n_splits, n_folds, unique_id)\n",
    "\n",
    "    # -- RETRAIN MODEL --------------------------------------------------------\n",
    "\n",
    "    print('Retraining model on full dataset')\n",
    "\n",
    "    model = HyperoptEstimator(classifier = clf,\n",
    "                              preprocessing = preprocessing,\n",
    "                              ex_preprocs = ex_preprocessing,\n",
    "                              algo = tpe.suggest,\n",
    "                              trial_timeout = timeout,\n",
    "                              loss_fn = f1_loss,\n",
    "                              max_evals = max_evals,\n",
    "                              n_jobs = n_jobs)\n",
    "\n",
    "    model.fit(X, y, n_folds=n_folds, cv_shuffle=True)\n",
    "\n",
    "    # -- PREDICT CLASSES FOR NEW DATA -----------------------------------------\n",
    "\n",
    "    print('Classifying new data')\n",
    "\n",
    "    y_pred = model.predict(measured_X) # Predict classes for measured TS(f)\n",
    "    y_pred = le.inverse_transform(y_pred) # Transform labels back to species\n",
    "\n",
    "    # -- OUTPUT RESULTS -------------------------------------------------------\n",
    "\n",
    "    mix_df['Prediction'] = y_pred\n",
    "    mix_df.to_pickle(unique_id + '_Predictions.pkl')\n",
    "\n",
    "    with open(unique_id + '_BestParams.pkl', 'wb') as handle:\n",
    "        pickle.dump(model.best_model(), handle)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07670fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 1 of 2\n",
      "100%|███████████████████████████████████████████████████████████████████| 1/1 [10:03<00:00, 603.47s/trial, best loss=?]\n",
      "Beginning fold 1 of 2\n",
      "100%|███████████████████████████████████████████████████████████████████| 1/1 [10:03<00:00, 603.81s/trial, best loss=?]\n",
      "Beginning fold 1 of 2\n",
      "  0%|                                                                            | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "main_classify(path, 'SED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff6791",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
