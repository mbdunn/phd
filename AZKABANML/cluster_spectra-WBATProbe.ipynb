{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f2db74",
   "metadata": {},
   "source": [
    "In this notebook we cluster the WBAT probe measured target spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c0113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from tenacity import retry, stop_after_attempt\n",
    "import glob\n",
    "import scipy as sc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage import feature\n",
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "\n",
    "import sys, errno  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe5ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96981ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Paired', n_colors=16)\n",
    "plt.style.use(['seaborn-colorblind'])\n",
    "plt.rc('font', family='Arial')\n",
    "SMALL_SIZE = 14\n",
    "BIGGER_SIZE = 15\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['ytick.left'] = True\n",
    "plt.rcParams['xtick.direction'] = 'inout'\n",
    "plt.rcParams['ytick.direction'] = 'inout'\n",
    "plt.rcParams['xtick.major.size'] = 5\n",
    "plt.rcParams['xtick.minor.size'] = 3\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['ytick.major.size'] = 5\n",
    "plt.rcParams['ytick.minor.size'] = 3\n",
    "plt.rcParams['ytick.major.width'] = 1\n",
    "\n",
    "savefigs_path = 'C:/Users/mbd/OneDrive - Akvaplan-niva AS/PhD-APN/ChaptersandExperiments/AFKABAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c261c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- USER-DETERMINED PARAMETERS -----------------------------------------------\n",
    "letter_drive = 'F'\n",
    "WBATpath = f'{letter_drive}:/AFKABAN/WBAT_data_PNC2023/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125efb2",
   "metadata": {},
   "source": [
    "# Read saved df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1980294",
   "metadata": {},
   "outputs": [],
   "source": [
    "WBAT_df = pd.read_feather(WBATpath+'/WBAT_tilt_df.feather')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ebd2f",
   "metadata": {},
   "source": [
    "# Cluster spectra\n",
    "sort by frequency bandwith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21005e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_ts_bandwidth(df):\n",
    "    'Function to seperated target spectra from 120 kHz and 200 kHz echosounders'\n",
    "    ind_120 = np.where(np.isnan(df['201.129']))[0]\n",
    "    ts_s_120 = np.where(df.columns.values=='94.032')[0][0] #remove 5% of spectra on either side b/c ramping\n",
    "    ts_e_120 = np.where(df.columns.values=='160.565')[0][0]\n",
    "    \n",
    "    ind_200 = np.where(np.isnan(df['120.242']))[0]\n",
    "    ts_s_200 = np.where(df.columns.values=='189.032')[0][0]\n",
    "    ts_e_200 = np.where(df.columns.values=='251.532')[0][0]\n",
    "\n",
    "    df_120 = df.iloc[ind_120,ts_s_120:ts_e_120]\n",
    "    df_200 = df.iloc[ind_200,ts_s_200:ts_e_200]\n",
    "    \n",
    "    return df_120, df_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c00bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "WBAT_df_120, WBAT_df_200 = select_ts_bandwidth(WBAT_df)\n",
    "\n",
    "WBAT_df_120 = WBAT_df_120.reset_index(drop=True)\n",
    "WBAT_df_200 = WBAT_df_200.reset_index(drop=True)\n",
    "\n",
    "WBAT_df_120.to_feather(f'{WBATpath}/WBAT_df_120.feather')\n",
    "WBAT_df_200.to_feather(f'{WBATpath}/WBAT_df_200.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682140b",
   "metadata": {},
   "source": [
    "### Select a df and run clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ee11db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmean_spectra(df, num_clus=6):\n",
    "    'Function to run kmean clusering on target spectra of a species'\n",
    "    # Set up kmeans\n",
    "    kmeans = KMeans(n_clusters=num_clus)\n",
    "    \n",
    "    # Normalize by the median of each target spectra\n",
    "    df_norm = df.div(df.max(axis=1), axis=0)\n",
    "    \n",
    "    # Fit data\n",
    "    y_kmean = kmeans.fit_predict(df_norm)\n",
    "\n",
    "    return y_kmean, kmeans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0975f2d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_79772\\4175331317.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_clus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mWBATs_120_ykmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmean_spectra\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWBATs_df_120\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_clus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_clus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mWBATs_200_ykmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmean_spectra\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWBATs_df_200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_clus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_clus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_79772\\1129563702.py\u001b[0m in \u001b[0;36mkmean_spectra\u001b[1;34m(df, num_clus)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Fit data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0my_kmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my_kmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \"\"\"\n\u001b[1;32m--> 996\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1365\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \"\"\"\n\u001b[1;32m-> 1367\u001b[1;33m         X = self._validate_data(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m             _assert_all_finite(\n\u001b[0m\u001b[0;32m    900\u001b[0m                 \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AZKABANML\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m                     \u001b[1;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                 )\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "num_clus=4\n",
    "\n",
    "WBATs_120_ykmean, k = kmean_spectra(WBATs_df_120, num_clus=num_clus)\n",
    "WBATs_200_ykmean, k = kmean_spectra(WBATs_df_200, num_clus=num_clus)\n",
    "\n",
    "WBATw_120_ykmean, k = kmean_spectra(WBATw_df_120, num_clus=num_clus)\n",
    "WBATw_200_ykmean, k = kmean_spectra(WBATw_df_200, num_clus=num_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f021e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add kmeans group to main dataframe\n",
    "def add_ykmean(df, y_120, y_200, df_f120, df_f200):\n",
    "    y_120 = pd.DataFrame(y_120, index=df_f120.index, columns=['y_kmean'])\n",
    "    y_200 = pd.DataFrame(y_200+6, index=df_f200.index, columns=['y_kmean'])\n",
    "    y = pd.concat([y_120,y_200])\n",
    "    df['y_kmean']=y\n",
    "    df['abs_tilt'] = np.abs(df['tilt'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "WBATstrong_df = add_ykmean(WBATstrong_df, WBATs_120_ykmean, WBATs_200_ykmean, WBATs_df_120, WBATs_df_200)\n",
    "\n",
    "WBATweak_df = add_ykmean(WBATweak_df, WBATw_120_ykmean, WBATw_200_ykmean, WBATw_df_120, WBATw_df_200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d858808",
   "metadata": {},
   "source": [
    "### Plot kmeans clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [WBATs_df_120,WBATw_df_120]\n",
    "y_list = [WBATs_120_ykmean, WBATw_120_ykmean]\n",
    "df2_list = [WBATs_df_200, WBATw_df_200]\n",
    "y2_list = [WBATs_200_ykmean, WBATw_200_ykmean]\n",
    "    \n",
    "name_list = ['Strong targets', 'Weak targets']\n",
    "fig,ax = plt.subplots(2,1, figsize=(8,8), sharex=True)\n",
    "axes = ax.flatten()\n",
    "N1 = np.zeros(num_clus)\n",
    "N2 = np.zeros(num_clus)\n",
    "\n",
    "for i_df in range(len(df_list)):\n",
    "    # 120 kHz\n",
    "    df1 = df_list[i_df]\n",
    "    y_kmean1 = y_list[i_df]\n",
    "    freqs_120 = df1.columns.unique().values.astype(np.float64)\n",
    "\n",
    "    for i_cluster in range(num_clus):\n",
    "        i_ts1 = np.where(y_kmean1==i_cluster)[0]\n",
    "        N1[i_cluster] = len(i_ts1)\n",
    "        \n",
    "    Nsort_ind = np.argsort(N1)\n",
    "    i_c = 0\n",
    "    for i_cluster in Nsort_ind:\n",
    "        \n",
    "        i_ts1 = np.where(y_kmean1==i_cluster)[0]\n",
    "        axes[i_df].plot(freqs_120,np.mean(df1.iloc[i_ts1,:].T,axis=1), c=palette[i_c], label=f'Cluster {i_cluster:d} ; {N1[i_cluster]*100/len(df1):.0f}%')\n",
    "        axes[i_df].fill_between(freqs_120,df1.iloc[i_ts1,:].quantile(q=0.25), df1.iloc[i_ts1,:].quantile(q=0.75), color=palette[i_c], alpha=0.1)\n",
    "        i_c += 1\n",
    "        \n",
    "    # 200 kHz\n",
    "    df2 = df2_list[i_df]\n",
    "    y_kmean2 = y2_list[i_df]\n",
    "    freqs_200 = df2.columns.unique().values.astype(np.float64)\n",
    "\n",
    "    for i_cluster in range(num_clus):\n",
    "        i_ts2 = np.where(y_kmean2==i_cluster)[0]\n",
    "        N2[i_cluster] = len(i_ts2) \n",
    "        \n",
    "    N2sort_ind = np.argsort(N2)\n",
    "    i_c = 0\n",
    "    for i_cluster in N2sort_ind:\n",
    "        i_ts2 = np.where(y_kmean2==i_cluster)[0]\n",
    "        axes[i_df].plot(freqs_200,np.mean(df2.iloc[i_ts2,:].T,axis=1), c=palette[i_c+num_clus], label=f'Cluster {i_cluster+num_clus:d} ; {N2[i_cluster]*100/len(df2):.0f}%')\n",
    "        axes[i_df].fill_between(freqs_200,df2.iloc[i_ts2,:].quantile(q=0.25), df2.iloc[i_ts2,:].quantile(q=0.75), color=palette[i_c+num_clus], alpha=0.1)\n",
    "        i_c +=1\n",
    "        \n",
    "    axes[i_df].legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=2, fontsize=12, frameon=True)\n",
    "    axes[i_df].set_title(f'{name_list[i_df]}', fontsize=15)\n",
    "    \n",
    "axes[0].set_xlabel('Frequency (kHz)', fontsize=15)\n",
    "axes[0].set_ylabel('Target Strength (dB re 1m$^2$)', fontsize=13)\n",
    "axes[1].set_ylabel('Target Strength (dB re 1m$^2$)', fontsize=13)\n",
    "\n",
    "#fig.savefig('C:/Users/mbd/OneDrive - Akvaplan-niva AS/PhD-APN/PhDdocs/ConferencesMeetings/WGFAST2023/clusters.jpeg', format='jpeg', dpi = 300, facecolor='w', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ffe099",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Atlanctic cod has a larger (~5dB) dip in the traget spectra that shifts horizontally. What causes the shift?\n",
    "- Polar cod (both experiments are similar, great start, has no consistent dips but positive, zero or negative slope.\n",
    "- Pandalus does not have enough targets for clustering. Too much variability for the few amount of tracked detections. But Pandalus can be differentiated with a threshold\n",
    "- clusters still have a lot of variability!\n",
    "\n",
    "Can we use to clusters as centroids of clusters for classification? \n",
    "\n",
    "Note: The clusters in 120 and 200 are not related! They have been clustered independently. Each tracked SED (calibrated beam compensared TS) is unaveraged or smoothed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b65bf",
   "metadata": {},
   "source": [
    "# Can we correlate cluster number to tilt angle??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0724778",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(8,4), sharey=True)\n",
    "axes = ax.flatten()\n",
    "sns.stripplot(WBATweak_df.iloc[WBATw_df_120.index.values,:], x='y_kmean', y='tilt', ax = axes[0], size=1)\n",
    "sns.stripplot(WBATweak_df.iloc[WBATw_df_200.index.values,:], x='y_kmean', y='tilt', ax = axes[1], size=1)\n",
    "\n",
    "axes[0].set(title = 'WBAT probe weak at 120 kHz', xlabel='Cluster ID', ylabel='Tilt (degrees)')\n",
    "axes[1].set(title = 'WBAT probe weak at 200 kHz', xlabel='Cluster ID', ylabel='Tilt (degrees)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96dbc9d",
   "metadata": {},
   "source": [
    "Cluster number and orientation does not seem to be related.\n",
    "\n",
    "Similar distributions for all species, both frequency bandwidths and each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40912c46",
   "metadata": {},
   "source": [
    "# Can we correlate cluster number to track number?? \n",
    "Do detection within the same track tend to be clustered together? If yes, it would show that broadband variability may be explained by individual fish length, material properties or other scattering properties that are relatively consistent for a single fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3abfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get region list for each frequency bandwidth\n",
    "def select_region_bandwidth(df):\n",
    "    'Function to seperated target spectra from 120 kHz and 200 kHz echosounders'\n",
    "    ind_120 = np.where(np.isnan(df['200.000']))[0]\n",
    "    ts_s_120 = np.where(df.columns.values=='90.000')[0][0]\n",
    "    ts_e_120 = np.where(df.columns.values=='160.000')[0][0]\n",
    "    \n",
    "    ind_200 = np.where(np.isnan(df['120.000']))[0]\n",
    "    ts_s_200 = np.where(df.columns.values=='185.000')[0][0]\n",
    "    ts_e_200 = np.where(df.columns.values=='255.000')[0][0]\n",
    "    \n",
    "    reg_120 = df['Region_name'][ind_120] \n",
    "    reg_200 = df['Region_name'][ind_200] \n",
    "    \n",
    "    return reg_120, reg_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e607f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_reg_120, a_reg_200 = select_region_bandwidth(WBATstrong_df)\n",
    "p_reg_120, p_reg_200 = select_region_bandwidth(WBATweak_df)\n",
    "p2_reg_120, p2_reg_200 = select_region_bandwidth(p2_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_reg_120.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74971e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset\n",
    "df_list = [a_df_120,a_df_200,p_df_120,p_df_200,p2_df_120,p2_df_200]\n",
    "y_list = [a_120_ykmean,a_200_ykmean, p_120_ykmean,p_200_ykmean, p2_120_ykmean,p2_200_ykmean]\n",
    "regions_list = df_list = [a_reg_120,a_reg_200,p_reg_120,p_reg_200,p2_reg_120,p2_reg_200]\n",
    "\n",
    "select = 5\n",
    "df=df_list[select]\n",
    "y_kmean = y_list[select]\n",
    "regions = regions_list[select]\n",
    "\n",
    "\n",
    "#---------\n",
    "all_regions = regions.unique()\n",
    "n_cluster_track = {}\n",
    "cluster_counts ={}\n",
    "n_cluster_ratio = []\n",
    "n_reg = []\n",
    "biggest_cluster_ratio = []\n",
    "\n",
    "count = 0\n",
    "for i_reg in all_regions:\n",
    "    ind_reg = np.where(regions==i_reg)[0]\n",
    "    if len(ind_reg)>2:\n",
    "        y_reg = y_kmean[ind_reg]\n",
    "\n",
    "        n_cluster_track[count], cluster_counts[count] = np.unique(y_reg,return_counts=True)\n",
    "        \n",
    "        biggest_cluster_ratio = np.append(biggest_cluster_ratio,max(cluster_counts[count])/len(ind_reg))\n",
    "\n",
    "        n_cluster_ratio = np.append(n_cluster_ratio,(len(n_cluster_track[count])/len(ind_reg)))\n",
    "        n_reg = np.append(n_reg, len(ind_reg))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f58cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2)\n",
    "axes = ax.flatten()\n",
    "axes[0].hist(n_cluster_ratio)\n",
    "\n",
    "axes[1].plot(n_reg,n_cluster_ratio, '.')\n",
    "axes[1].set(xlabel='Number of detections in track',ylabel='Consistency');\n",
    "\n",
    "axes[2].hist(biggest_cluster_ratio)\n",
    "\n",
    "axes[3].plot(n_reg,n_cluster_ratio*n_reg, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64905b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3a7c6f8",
   "metadata": {},
   "source": [
    "#### How many tracks contain one cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5661b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(n_cluster_track)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee94036",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster_track[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a2e237",
   "metadata": {},
   "source": [
    "Haven't proved it yet but it really doesnt seem to be a driving factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff5640",
   "metadata": {},
   "source": [
    "## Try pca? in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c5ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder() # Maps labels -> int (e.g. Atlantic cod -> 0, Polar cod -> 1)\n",
    "a_df['RegionID'] = le.fit_transform(a_df.Region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = np.array(a_df[['tilt','RegionID','y_kmean','abs_tilt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    " \n",
    "df_scaled = std_scaler.fit_transform(sub_df['tilt'])\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=[\n",
    "  'tilt','RegionID','y_kmean','abs_tilt'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137184de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_scaled[~np.any(np.isnan(sub_df),axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f1067",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f85f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "n_samples = len(X)\n",
    "pca = PCA(n_components=4).fit(X)\n",
    "\n",
    "\n",
    "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], alpha=0.3, label=\"samples\")\n",
    "for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\n",
    "    comp = comp * var  # scale component by its variance explanation power\n",
    "    plt.plot(\n",
    "        [0, comp[0]],\n",
    "        [0, comp[1]],\n",
    "        label=f\"Component {X.columns[i]}\",\n",
    "        linewidth=5,\n",
    "        color=f\"C{i + 2}\",\n",
    "    )\n",
    "plt.gca().set(\n",
    "    aspect=\"equal\",\n",
    "    title=\"2-dimensional dataset with principal components\",\n",
    "    xlabel=\"first feature\",\n",
    "    ylabel=\"second feature\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726309d",
   "metadata": {},
   "source": [
    "# Storage Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b343338",
   "metadata": {},
   "source": [
    "### Elbow algorithm to find optimal clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1107c",
   "metadata": {},
   "source": [
    "Run elbow algorithm for optimal number of clusters.\n",
    "\n",
    "Optimal value is found at the \"elbow\" of the distortion results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a90e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Use the quick method and immediately show the figure\n",
    "df = WBATs_df_120\n",
    "df_norm = df.div(df.median(axis=1), axis=0)\n",
    "kelbow_visualizer(KMeans(), df_norm, k=(1,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741ec85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
